# Defaults for pretraining with train.py.
#
# You must also include a binding for MODEL.
#
# Required to be set:
#
# -MIXTURE_OR_TASK_NAME
# -MIXTURE_OR_TASK_MODULE
# -TASK_FEATURE_LENGTHS
# -train.model_dir
#
# Commonly overridden options:
#
# - DatasetConfig.batch_size
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
#
# Currently we don't support inference eval.
from __gin__ import dynamic_registration

import __main__ as train_script

from t5x import adafactor
from t5x import partitioning
from t5x import utils
from t5x_retrieval import adafactor_utils
from t5x_retrieval import partitioning as t5xr_partitioning

include 't5x/configs/runs/pretrain.gin'

train_script.train:
  infer_eval_dataset_cfg = None

train/utils.DatasetConfig:
  use_cached = False
  pack = False

train_eval/utils.DatasetConfig:
  use_cached = False
  pack = False

utils.create_learning_rate_scheduler:
  factors = 'linear_decay'
  base_learning_rate = 0.001
  warmup_steps = 1000
  decay_factor = 0.0001 # 1 / %TRAIN_STEPS

adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @adafactor_utils.logical_factor_rules()

partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @t5xr_partitioning.standard_logical_axis_rules()
